{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12564182,"sourceType":"datasetVersion","datasetId":7811352},{"sourceId":12709214,"sourceType":"datasetVersion","datasetId":7887000}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install libraries","metadata":{}},{"cell_type":"code","source":"# %%capture\n# !pip install emoji regex pandas unicodedata fasttext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.058504Z","iopub.execute_input":"2025-08-08T10:38:12.058806Z","iopub.status.idle":"2025-08-08T10:38:12.063893Z","shell.execute_reply.started":"2025-08-08T10:38:12.058785Z","shell.execute_reply":"2025-08-08T10:38:12.063209Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport regex\nimport emoji\nimport unicodedata\nimport uuid\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.065033Z","iopub.execute_input":"2025-08-08T10:38:12.065280Z","iopub.status.idle":"2025-08-08T10:38:12.395805Z","shell.execute_reply.started":"2025-08-08T10:38:12.065238Z","shell.execute_reply":"2025-08-08T10:38:12.394851Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# df = pd.read_csv(\"/kaggle/input/eureka-version-dataset/final_raw.csv\")\n# df = pd.read_csv(\"/kaggle/input/eureka-version-dataset/v2_raw.csv\")\ndf = pd.read_csv(\"/kaggle/input/eureka-version-dataset/final_raw_new.csv\")\nselected_df = df[['summary', 'comment_raw', 'label']]\nprint(\"======== Head ========\")\nprint(selected_df.head(5))\nprint(\"\\n======== Tail ========\")\nprint(selected_df.tail(5))\n\nprint(\"\\n======== Shape ========\")\nprint(selected_df.shape)\n\nprint(\"\\n======== Info ========\")\nprint(selected_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.396611Z","iopub.execute_input":"2025-08-08T10:38:12.396936Z","iopub.status.idle":"2025-08-08T10:38:12.782459Z","shell.execute_reply.started":"2025-08-08T10:38:12.396916Z","shell.execute_reply":"2025-08-08T10:38:12.781758Z"}},"outputs":[{"name":"stdout","text":"======== Head ========\n                                             summary  \\\n0  1. N·ªôi dung s∆° l∆∞·ª£c: B√†i vi·∫øt ch·ªâ tr√≠ch Ph·∫°m V...   \n1  1. N·ªôi dung s∆° l∆∞·ª£c: B√†i vi·∫øt ch·ªâ tr√≠ch Ph·∫°m V...   \n2  1. N·ªôi dung s∆° l∆∞·ª£c: B√†i vi·∫øt ch·ªâ tr√≠ch Ph·∫°m V...   \n3  1. N·ªôi dung s∆° l∆∞·ª£c: B√†i vi·∫øt ch·ªâ tr√≠ch Ph·∫°m V...   \n4  1. N·ªôi dung s∆° l∆∞·ª£c: B√†i vi·∫øt ch·ªâ tr√≠ch Ph·∫°m V...   \n\n                                         comment_raw            label  \n0  lu·∫≠n ƒëi·ªáu c·ªßa b·ªçn ph·∫£n ƒë·ªông, s·ªè l√°, 3/// vi·∫øt ...  KHONG_PHAN_DONG  \n1  v·∫≠y √¥ng b·∫£o ƒë·∫°i, √¥ng di·ªám, √¥ng thi·ªáu l√† ƒë·∫£ng v...  KHONG_PHAN_DONG  \n2                              mu√¥n ƒë·ªùi c·ªßa ƒë√°m 3///  KHONG_PHAN_DONG  \n3  gi√† r·ªìi m√† ƒë·∫ßn v·∫≠y ch√°u ? c·ªông s·∫£n ƒë√°nh m·ªπ, ƒë√°...  KHONG_PHAN_DONG  \n4  ƒë√∫ng l√† 3/// x·ªè l√°, b√°c h·ªì m·∫•t n√™n c√°c b√°c kh√≥...  KHONG_PHAN_DONG  \n\n======== Tail ========\n                                                 summary  \\\n17646  1. N·ªôi dung s∆° l∆∞·ª£c: C√¢u chuy·ªán ng·ª• ng√¥n v·ªÅ cu...   \n17647  1. N·ªôi dung s∆° l∆∞·ª£c: C√¢u chuy·ªán ng·ª• ng√¥n v·ªÅ cu...   \n17648  1. N·ªôi dung s∆° l∆∞·ª£c: C√¢u chuy·ªán ng·ª• ng√¥n v·ªÅ cu...   \n17649  1. N·ªôi dung s∆° l∆∞·ª£c: C√¢u chuy·ªán ng·ª• ng√¥n v·ªÅ cu...   \n17650  1. N·ªôi dung s∆° l∆∞·ª£c: C√¢u chuy·ªán ng·ª• ng√¥n v·ªÅ cu...   \n\n                                             comment_raw            label  \n17646  √¥ng b√† n·ªôi b·∫°n b·ªã ch√¥n s·ªëng th√¨ t·ªõi ƒë·ªùi con ch...  KHONG_LIEN_QUAN  \n17647                        v√¨ ƒë√†i d√°m n√≥i l√™n s·ª± th·∫≠t?  KHONG_LIEN_QUAN  \n17648  s·ª± th·∫≠t g√¨? b√°o ƒë√†i ng√†y n√†o ch·∫£ ƒëƒÉng! th·∫≠t c√°...  KHONG_LIEN_QUAN  \n17649                   s·ª± th·∫≠t nh∆∞ nick c·ªßa ch√°u v·∫≠y ƒë√≥  KHONG_LIEN_QUAN  \n17650  con hoang ƒë∆∞·ª£c tr·∫°i tr·∫ª m·ªì c√¥i nh·∫≠n nu√¥i n√≥ c√≤...  KHONG_LIEN_QUAN  \n\n======== Shape ========\n(17651, 3)\n\n======== Info ========\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 17651 entries, 0 to 17650\nData columns (total 3 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   summary      17651 non-null  object\n 1   comment_raw  17651 non-null  object\n 2   label        17651 non-null  object\ndtypes: object(3)\nmemory usage: 413.8+ KB\nNone\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 1. Normalize Unicode (NFC) + lowercase","metadata":{}},{"cell_type":"code","source":"def normalize_unicode_lower(text):\n    text = unicodedata.normalize('NFC', text)\n    return text.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.783750Z","iopub.execute_input":"2025-08-08T10:38:12.783966Z","iopub.status.idle":"2025-08-08T10:38:12.787609Z","shell.execute_reply.started":"2025-08-08T10:38:12.783949Z","shell.execute_reply":"2025-08-08T10:38:12.787004Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## 2. Remove Emoji, links/HTML tags/mentions/hashtags/UI indicators","metadata":{}},{"cell_type":"code","source":"# emoji\nEMOTICON_PATTERNS = [\n    r\":\\)+\",       # :), :)), :))), ...\n    r\":\\(+\",       # :(, :((, ...\n    r\":v+\",        # :v, :vvv, ...\n    r\":V+\",        # :V, :VV, ...\n    r\"=+\\)+\",      # =), =)), ...\n    r\"=+\\(+\",      # =(, =((, ...\n    r\":d+\",        # :d, :dd\n    r\":p+\",        # :p, :pp\n    r\"<3+\",        # <3<3<3\n    r\"=+\\]+\",      # =], =]], =]]], ...\n    r\"=+\\[+\",      # =[, =[[, =[[[ ...\n    r\":>+\",        # :>, :>>, ...\n    r\":<+\",        # :<, :<<, ...\n    r\":\\(\\(\",      # :((\n    r\"=\\(\\(\",      # =((\n]\n\nEMOTICON_REGEX = re.compile(\"|\".join(EMOTICON_PATTERNS), re.IGNORECASE)\n\nEMOJI_REGEX = re.compile(\n    \"[\"\n    u\"\\U0001F600-\\U0001F64F\"\n    u\"\\U0001F300-\\U0001F5FF\"\n    u\"\\U0001F680-\\U0001F6FF\"\n    u\"\\U0001F700-\\U0001F77F\"\n    u\"\\U0001F780-\\U0001F7FF\"\n    u\"\\U0001F800-\\U0001F8FF\"\n    u\"\\U0001F900-\\U0001F9FF\"\n    u\"\\U0001FA00-\\U0001FA6F\"\n    u\"\\U0001FA70-\\U0001FAFF\"\n    u\"\\U00002702-\\U000027B0\"\n    u\"\\U000024C2-\\U0001F251\" \n    \"]+\", flags=re.UNICODE\n)\n\ndef remove_emoji_emoticon(text):    \n    try:\n        text = emoji.replace_emoji(text, replace=\" \")\n    except:\n        text = EMOJI_REGEX.sub(\" \", text)\n    \n    # X√≥a b·∫±ng regex\n    text = EMOTICON_REGEX.sub(\" \", text)\n    \n    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n# url, html, mention, hashtags, ui_indicators\nURL_REGEX = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|co|vn|io)(/\\S*)?')\nHTML_REGEX = re.compile(r\"<[^>]+>\")\nMENTION_REGEX = re.compile(r\"@[\\w\\._]+\")\nHASHTAG_REGEX = re.compile(r\"#\\w+\")\nUI_INDICATORS = [\n    \"ƒë√£ ch·ªânh s·ª≠a\", \"[ƒë√£ ch·ªânh s·ª≠a]\", \"(ƒë√£ ch·ªânh s·ª≠a)\",\n    \"see more\", \"xem th√™m\", \"see translation\", \"xem b·∫£n d·ªãch\",\n    \"·∫©n b·ªõt\", \"xem √≠t h∆°n\", \"d·ªãch\", \"translated\", \"more\", \"less\",\n    \"see more reactions\"\n]\n\ndef remove_html_url_mention_hashtag(text):\n    if not isinstance(text, str):\n        return \"\"\n\n    # X√≥a URL\n    text = URL_REGEX.sub(\" \", text)\n    \n    # X√≥a HTML tags\n    text = HTML_REGEX.sub(\" \", text)\n    \n    # X√≥a mentions v√† hashtags\n    text = MENTION_REGEX.sub(\" \", text)\n    text = HASHTAG_REGEX.sub(\" \", text)\n    \n    # X√≥a ui_indicators\n    for ind in UI_INDICATORS:\n        text = re.sub(r'(?i)' + re.escape(ind), \" \", text)\n    \n    # Lo·∫°i b·ªè d·∫•u c√¢u ri√™ng l·∫ª\n    text = re.sub(r'(?<!\\w)[\\^\\'\\`\\~\\\"\\,\\.]+(?!\\w)', ' ', text)\n    \n    # L√†m s·∫°ch kho·∫£ng tr·∫Øng d∆∞ th·ª´a\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.788487Z","iopub.execute_input":"2025-08-08T10:38:12.788709Z","iopub.status.idle":"2025-08-08T10:38:12.811820Z","shell.execute_reply.started":"2025-08-08T10:38:12.788684Z","shell.execute_reply":"2025-08-08T10:38:12.811056Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 3. Reduce elongated characters","metadata":{}},{"cell_type":"code","source":"def reduce_elongated(text):\n    if not isinstance(text, str):\n        return \"\"\n    pattern = regex.compile(r\"([\\p{L}])\\1{2,}\", flags=regex.IGNORECASE)\n    return pattern.sub(r\"\\1\\1\", text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.812591Z","iopub.execute_input":"2025-08-08T10:38:12.812810Z","iopub.status.idle":"2025-08-08T10:38:12.829665Z","shell.execute_reply.started":"2025-08-08T10:38:12.812791Z","shell.execute_reply":"2025-08-08T10:38:12.828948Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 4. Lexical Normalization","metadata":{}},{"cell_type":"code","source":"import json, re\n\n# --- Load dict and build patterns ---\nwith open('/kaggle/input/dictionary/abbreviation_dictionary.json', encoding='utf-8') as f:\n    norm_dict = json.load(f)\n\nmixed, pure, words = [], [], []\nfor slang, std in norm_dict.items():\n    esc = re.escape(slang)\n    # classification\n    if re.fullmatch(r\"[^\\w\\s]+\", slang):\n        pure.append((esc, std))\n    elif re.search(r\"[^\\w\\s]\", slang) and re.search(r\"\\w\", slang):\n        mixed.append((esc, std))\n    else:\n        # add \\b for normal words\n        words.append((rf\"\\b{esc}\\b\", std))\n\n# sort desc\nfor lst in (mixed, pure, words):\n    lst.sort(key=lambda x: -len(x[0].replace(r\"\\b\",\"\")))\n\n# compile \n_patterns = [\n    (re.compile(pat, flags=re.IGNORECASE), std)\n    for pat, std in (mixed + pure + words)\n]\n\ndef apply_lexical_normalization(text):\n    if not isinstance(text, str):\n        return text\n    for regex, std in _patterns:\n        text = regex.sub(std, text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.830471Z","iopub.execute_input":"2025-08-08T10:38:12.830808Z","iopub.status.idle":"2025-08-08T10:38:12.869920Z","shell.execute_reply.started":"2025-08-08T10:38:12.830784Z","shell.execute_reply":"2025-08-08T10:38:12.869431Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 4.1. Debug lexical normalization ","metadata":{}},{"cell_type":"code","source":"# import json, re\n\n# # --- Load dict and build patterns ---\n# with open('/kaggle/input/dictionary/abbreviation_dictionary.json', encoding='utf-8') as f:\n#     norm_dict = json.load(f)\n\n# # --- Th√™m tracking cho t·ª´ kh√≥a ch√≠nh tr·ªã nh·∫°y c·∫£m ---\n# POLITICAL_KEYWORDS = [\n#     'b√≤ ƒë·ªè', 'ba que', 'ph·∫£n ƒë·ªông', 'd∆∞ lu·∫≠n vi√™n',\n#     'c·ªông s·∫£n', 'vi·ªát c·ªông', 'ƒë·ªôc t√†i', 'ƒë·∫£ng',\n#     'vi·ªát nam c·ªông h√≤a', 'b·∫Øc k·ª≥', 'nam k·ª≥',\n#     'trung qu·ªëc', 't√†u c·ªông', 'y√™u n∆∞·ªõc', 't·ª± do', 'nh√¢n quy·ªÅn',\n#     # Th√™m c√°c bi·∫øn th·ªÉ c√≥ th·ªÉ c√≥ trong dictionary\n#     'cs', 'vc', 'vnch', 'tc', 'pd', 'dlv'\n# ]\n\n# # Tracking dict ƒë·ªÉ ghi l·∫°i c√°c transformations\n# political_transformations = {}\n# for keyword in POLITICAL_KEYWORDS:\n#     if keyword in norm_dict:\n#         political_transformations[keyword] = norm_dict[keyword]\n\n# print(\"=== POLITICAL KEYWORDS NORMALIZATION TRACKING ===\")\n# print(f\"Total dictionary entries: {len(norm_dict):,}\")\n# print(f\"Political keywords to track: {len(POLITICAL_KEYWORDS)}\")\n# print(f\"Political keywords found in dictionary: {len(political_transformations)}\")\n\n# if political_transformations:\n#     print(\"\\nPolitical keyword transformations:\")\n#     for original, normalized in political_transformations.items():\n#         print(f\"  '{original}' ‚Üí '{normalized}'\")\n# else:\n#     print(\"\\nNo political keywords found in normalization dictionary\")\n\n# # Ki·ªÉm tra c√°c t·ª´ kh√≥a c√≥ pattern t∆∞∆°ng t·ª±\n# similar_keys = []\n# for key in norm_dict.keys():\n#     for pol_word in POLITICAL_KEYWORDS:\n#         if pol_word.lower() in key.lower() or key.lower() in pol_word.lower():\n#             if key not in political_transformations:\n#                 similar_keys.append((key, norm_dict[key]))\n\n# if similar_keys:\n#     print(f\"\\nSimilar/related entries found: {len(similar_keys)}\")\n#     for original, normalized in similar_keys[:10]:  # Hi·ªÉn th·ªã top 10\n#         print(f\"  '{original}' ‚Üí '{normalized}'\")\n#     if len(similar_keys) > 10:\n#         print(f\"  ... and {len(similar_keys) - 10} more\")\n\n# mixed, pure, words = [], [], []\n# for slang, std in norm_dict.items():\n#     esc = re.escape(slang)\n#     # classification\n#     if re.fullmatch(r\"[^\\w\\s]+\", slang):\n#         pure.append((esc, std))\n#     elif re.search(r\"[^\\w\\s]\", slang) and re.search(r\"\\w\", slang):\n#         mixed.append((esc, std))\n#     else:\n#         # add \\b for normal words\n#         words.append((rf\"\\b{esc}\\b\", std))\n\n# # sort desc\n# for lst in (mixed, pure, words):\n#     lst.sort(key=lambda x: -len(x[0].replace(r\"\\b\",\"\")))\n\n# # compile \n# _patterns = [\n#     (re.compile(pat, flags=re.IGNORECASE), std)\n#     for pat, std in (mixed + pure + words)\n# ]\n\n# def apply_lexical_normalization(text):\n#     if not isinstance(text, str):\n#         return text\n#     for regex, std in _patterns:\n#         text = regex.sub(std, text)\n#     return text\n\n# print(\"=== PATTERN COMPILATION COMPLETE ===\")\n# print(f\"Mixed patterns: {len(mixed)}\")\n# print(f\"Pure punctuation patterns: {len(pure)}\")\n# print(f\"Word patterns: {len(words)}\")\n# print(f\"Total compiled patterns: {len(_patterns)}\")\n# print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.870557Z","iopub.execute_input":"2025-08-08T10:38:12.870748Z","iopub.status.idle":"2025-08-08T10:38:12.875509Z","shell.execute_reply.started":"2025-08-08T10:38:12.870732Z","shell.execute_reply":"2025-08-08T10:38:12.874885Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 5. Remove punctuation","metadata":{}},{"cell_type":"code","source":"VIET_CHARACTERS = (\n    \"√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠\"\n    \"√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá\"\n    \"√¨√≠·ªâƒ©·ªã\"\n    \"√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£\"\n    \"√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±\"\n    \"·ª≥√Ω·ª∑·ªπ·ªµ\"\n    \"ƒë\"\n)\n\ndef remove_punctuation(text):\n    if not isinstance(text, str):\n        return \"\"\n    # Gi·ªØ ch·ªØ Vi·ªát + ch·ªØ Anh + s·ªë + space\n    text = regex.sub(rf\"[^{VIET_CHARACTERS}a-zA-Z0-9\\s]+\", \" \", text)\n    return regex.sub(r\"\\s+\", \" \", text).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.877519Z","iopub.execute_input":"2025-08-08T10:38:12.877925Z","iopub.status.idle":"2025-08-08T10:38:12.891047Z","shell.execute_reply.started":"2025-08-08T10:38:12.877908Z","shell.execute_reply":"2025-08-08T10:38:12.890444Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 6. Whitespace Stripping ","metadata":{}},{"cell_type":"code","source":"def strip_extra_spaces(text):\n    if not isinstance(text, str):\n        return \"\"\n    return regex.sub(r\"\\s+\", \" \", text).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.891634Z","iopub.execute_input":"2025-08-08T10:38:12.891823Z","iopub.status.idle":"2025-08-08T10:38:12.903685Z","shell.execute_reply.started":"2025-08-08T10:38:12.891799Z","shell.execute_reply":"2025-08-08T10:38:12.903091Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 7. Deduplication ","metadata":{}},{"cell_type":"code","source":"def deduplicate_comments(df, col):\n    before = len(df)\n    df_nodup = df.drop_duplicates(subset=[col]).reset_index(drop=True)\n    after = len(df_nodup)\n    return df_nodup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.904428Z","iopub.execute_input":"2025-08-08T10:38:12.904656Z","iopub.status.idle":"2025-08-08T10:38:12.916481Z","shell.execute_reply.started":"2025-08-08T10:38:12.904641Z","shell.execute_reply":"2025-08-08T10:38:12.915894Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"selected_df = selected_df.copy()\noriginal_count = len(selected_df)\nprint(f\"Starting with {original_count:,} comments\")\n\n# 1. Unicode normalization + lowercase\nprint(\"1. Normalizing Unicode and converting to lowercase\")\nselected_df['comment_clean'] = selected_df['comment_raw'].apply(normalize_unicode_lower)\n\n# 2. Remove Emoji, links/HTML/mentions/hashtags/UI indicators\nprint(\"2. Removing Remove Emoji,links/HTML/mentions/hashtags/UI indicators\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_emoji_emoticon)\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_html_url_mention_hashtag)\n\n# 3. Reduce elongated characters\nprint(\"3. Reducing elongated characters\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(reduce_elongated)\n\n# 4. Lexical normalization\n# print(\"4. Applying lexical normalization\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(apply_lexical_normalization)\nprint(\"4. Applying lexical normalization\")\nbefore = selected_df['comment_clean'].copy()\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(apply_lexical_normalization)\n\nnum_lines_changed = (before != selected_df['comment_clean']).sum()\npercent_lines_changed = (num_lines_changed / len(selected_df)) * 100 if len(selected_df) else 0\n\nprint(f\"  S·ªë d√≤ng ƒë√£ ch·ªânh s·ª≠a: {num_lines_changed:,}/{len(selected_df):,} ({percent_lines_changed:.2f}%)\")\n# 5. Remove punctuation\nprint(\"5. Removing all punctuation\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_punctuation)\n\n# 6. Whitespace Stripping\nprint(\"6. Whitespace Stripping\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(strip_extra_spaces)\n\n# 7. Deduplication\nprint(\"7. Removing duplicate comments\")\nbefore_dedup = len(selected_df)\nselected_df = deduplicate_comments(selected_df, col='comment_clean')\nafter_dedup = len(selected_df)\nprint(f\"  Removed {before_dedup - after_dedup:,} duplicate comments\")\n\n# Statistics Summary\nprint(\"\\n\" + \"=\"*50)\nfinal_count = len(selected_df)\ntotal_reduction = original_count - final_count\nretention_rate = (final_count / original_count) * 100\nreduction_rate = (total_reduction / original_count) * 100\n\nprint(f\"Original comments:      {original_count:,}\")\nprint(f\"After filtering:        {original_count:,} (100.0%)\")\nprint(f\"Final comments:         {final_count:,} ({retention_rate:.1f}%)\")\nprint(f\"Total reduction:        {total_reduction} comments ({reduction_rate:.1f}%)\")\n\n# Label Distribution\nif 'label' in selected_df.columns:\n    print(f\"\\n Label Distribution:\")\n    label_counts = selected_df['label'].value_counts().sort_index()\n    \n    for label, count in label_counts.items():\n        percentage = (count / final_count) * 100\n        print(f\"  ‚Ä¢ {label:<18}: {count:,} ({percentage:.1f}%)\")\n\nprint(\"=\"*50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.917127Z","iopub.execute_input":"2025-08-08T10:38:12.917359Z","iopub.status.idle":"2025-08-08T10:38:52.629160Z","shell.execute_reply.started":"2025-08-08T10:38:12.917334Z","shell.execute_reply":"2025-08-08T10:38:52.628529Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Starting with 17,651 comments\n1. Normalizing Unicode and converting to lowercase\n2. Removing Remove Emoji,links/HTML/mentions/hashtags/UI indicators\n3. Reducing elongated characters\n4. Applying lexical normalization\n  S·ªë d√≤ng ƒë√£ ch·ªânh s·ª≠a: 9,517/17,651 (53.92%)\n5. Removing all punctuation\n6. Whitespace Stripping\n7. Removing duplicate comments\n  Removed 350 duplicate comments\n\n==================================================\nOriginal comments:      17,651\nAfter filtering:        17,651 (100.0%)\nFinal comments:         17,301 (98.0%)\nTotal reduction:        350 comments (2.0%)\n\n Label Distribution:\n  ‚Ä¢ KHONG_LIEN_QUAN   : 8,886 (51.4%)\n  ‚Ä¢ KHONG_PHAN_DONG   : 6,202 (35.8%)\n  ‚Ä¢ PHAN_DONG         : 2,213 (12.8%)\n==================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"selected_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:52.629960Z","iopub.execute_input":"2025-08-08T10:38:52.630223Z","iopub.status.idle":"2025-08-08T10:38:52.642981Z","shell.execute_reply.started":"2025-08-08T10:38:52.630199Z","shell.execute_reply":"2025-08-08T10:38:52.642093Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 17301 entries, 0 to 17300\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   summary        17301 non-null  object\n 1   comment_raw    17301 non-null  object\n 2   label          17301 non-null  object\n 3   comment_clean  17301 non-null  object\ndtypes: object(4)\nmemory usage: 540.8+ KB\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### DEBUG","metadata":{}},{"cell_type":"code","source":"# selected_df = selected_df.copy()\n# original_count = len(selected_df)\n# print(f\"Starting with {original_count:,} comments\")\n\n# # 1. Unicode normalization + lowercase\n# print(\"1. Normalizing Unicode and converting to lowercase\")\n# selected_df['comment_clean'] = selected_df['comment_raw'].apply(normalize_unicode_lower)\n\n# # 2. Remove Emoji, links/HTML/mentions/hashtags/UI indicators\n# print(\"2. Removing Remove Emoji,links/HTML/mentions/hashtags/UI indicators\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_emoji_emoticon)\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_html_url_mention_hashtag)\n\n# # 3. Reduce elongated characters\n# print(\"3. Reducing elongated characters\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(reduce_elongated)\n\n# # 4. Lexical normalization v·ªõi tracking chi ti·∫øt\n# print(\"4. Applying lexical normalization\")\n# before = selected_df['comment_clean'].copy()\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(apply_lexical_normalization)\n\n# num_lines_changed = (before != selected_df['comment_clean']).sum()\n# percent_lines_changed = (num_lines_changed / len(selected_df)) * 100 if len(selected_df) else 0\n\n# print(f\"  S·ªë d√≤ng ƒë√£ ch·ªânh s·ª≠a: {num_lines_changed:,}/{len(selected_df):,} ({percent_lines_changed:.2f}%)\")\n\n# # Th√™m tracking cho political keywords\n# print(\"\\n  === POLITICAL KEYWORDS TRACKING ===\")\n# political_keyword_stats = {}\n\n# for keyword in POLITICAL_KEYWORDS:\n#     before_count = before.str.contains(keyword, case=False, na=False).sum()\n#     after_count = selected_df['comment_clean'].str.contains(keyword, case=False, na=False).sum()\n    \n#     if before_count > 0 or after_count > 0:\n#         political_keyword_stats[keyword] = {\n#             'before': before_count,\n#             'after': after_count,\n#             'change': after_count - before_count\n#         }\n\n# if political_keyword_stats:\n#     print(f\"  Political keywords found in dataset:\")\n#     for keyword, stats in political_keyword_stats.items():\n#         if stats['before'] > 0 or stats['after'] > 0:\n#             change_sign = \"+\" if stats['change'] > 0 else \"\"\n#             print(f\"    '{keyword}': {stats['before']} ‚Üí {stats['after']} ({change_sign}{stats['change']})\")\n# else:\n#     print(\"  No tracked political keywords found in dataset\")\n\n# # Ki·ªÉm tra t·ª´ kh√≥a ƒë∆∞·ª£c normalize th√†nh g√¨\n# if political_transformations:\n#     print(f\"\\n  Checking normalized political keywords:\")\n#     for original, normalized in political_transformations.items():\n#         normalized_count = selected_df['comment_clean'].str.contains(normalized, case=False, na=False).sum()\n#         if normalized_count > 0:\n#             print(f\"    '{normalized}' (from '{original}'): {normalized_count} occurrences\")\n\n# print(\"  =\" * 40)\n\n# # 5. Remove punctuation\n# print(\"5. Removing all punctuation\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_punctuation)\n\n# # 6. Whitespace Stripping\n# print(\"6. Whitespace Stripping\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(strip_extra_spaces)\n\n# # 7. Deduplication\n# print(\"7. Removing duplicate comments\")\n# before_dedup = len(selected_df)\n# selected_df = deduplicate_comments(selected_df, col='comment_clean')\n# after_dedup = len(selected_df)\n# print(f\"  Removed {before_dedup - after_dedup:,} duplicate comments\")\n\n# # Statistics Summary\n# print(\"\\n\" + \"=\"*50)\n# final_count = len(selected_df)\n# total_reduction = original_count - final_count\n# retention_rate = (final_count / original_count) * 100\n# reduction_rate = (total_reduction / original_count) * 100\n\n# print(f\"Original comments:      {original_count:,}\")\n# print(f\"After filtering:        {original_count:,} (100.0%)\")\n# print(f\"Final comments:         {final_count:,} ({retention_rate:.1f}%)\")\n# print(f\"Total reduction:        {total_reduction} comments ({reduction_rate:.1f}%)\")\n\n# # Label Distribution\n# if 'label' in selected_df.columns:\n#     print(f\"\\n Label Distribution:\")\n#     label_counts = selected_df['label'].value_counts().sort_index()\n    \n#     for label, count in label_counts.items():\n#         percentage = (count / final_count) * 100\n#         print(f\"  ‚Ä¢ {label:<18}: {count:,} ({percentage:.1f}%)\")\n\n# # Final political keywords summary v·ªõi breakdown theo label\n# print(f\"\\n=== FINAL POLITICAL KEYWORDS SUMMARY ===\")\n# final_political_stats = {}\n# for keyword in POLITICAL_KEYWORDS:\n#     count = selected_df['comment_clean'].str.contains(keyword, case=False, na=False).sum()\n#     if count > 0:\n#         final_political_stats[keyword] = count\n\n# if final_political_stats:\n#     print(\"Political keywords in final dataset:\")\n#     for keyword, count in sorted(final_political_stats.items(), key=lambda x: x[1], reverse=True):\n#         percentage = (count / final_count) * 100\n#         print(f\"  '{keyword}': {count:,} occurrences ({percentage:.2f}%)\")\n        \n#         # Breakdown by label\n#         if 'label' in selected_df.columns:\n#             for label in selected_df['label'].unique():\n#                 label_count = selected_df[selected_df['label'] == label]['comment_clean'].str.contains(keyword, case=False, na=False).sum()\n#                 if label_count > 0:\n#                     label_total = len(selected_df[selected_df['label'] == label])\n#                     label_percentage = (label_count / label_total) * 100\n#                     print(f\"    - {label}: {label_count:,} ({label_percentage:.2f}%)\")\n# else:\n#     print(\"No political keywords found in final dataset\")\n\n# print(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:52.643988Z","iopub.execute_input":"2025-08-08T10:38:52.644331Z","iopub.status.idle":"2025-08-08T10:38:52.659010Z","shell.execute_reply.started":"2025-08-08T10:38:52.644302Z","shell.execute_reply":"2025-08-08T10:38:52.658308Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Export\noutput_path = \"/kaggle/working/final_claened_new.csv\"\nselected_df.to_csv(output_path, index=False)\nprint(f\"Cleaned comments exported to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:52.659889Z","iopub.execute_input":"2025-08-08T10:38:52.660139Z","iopub.status.idle":"2025-08-08T10:38:53.086793Z","shell.execute_reply.started":"2025-08-08T10:38:52.660114Z","shell.execute_reply":"2025-08-08T10:38:53.086018Z"}},"outputs":[{"name":"stdout","text":"Cleaned comments exported to: /kaggle/working/final_claened_new.csv\n","output_type":"stream"}],"execution_count":15},{"cell_type":"raw","source":"## rename","metadata":{},"attachments":{}},{"cell_type":"code","source":"# # rename comment_clean -> comment\n# export_df = selected_df[['summary', 'comment_clean', 'label']] \\\n#     .rename(columns={'comment_clean': 'comment'})\n\n# output_path = \"/kaggle/working/v2_clean_1.csv\"\n# export_df.to_csv(output_path, index=False)\n\n# print(f\"Cleaned comments exported to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:53.087661Z","iopub.execute_input":"2025-08-08T10:38:53.087997Z","iopub.status.idle":"2025-08-08T10:38:53.091689Z","shell.execute_reply.started":"2025-08-08T10:38:53.087972Z","shell.execute_reply":"2025-08-08T10:38:53.090913Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# # test\n# # DEBUG\n# test_comments = [\n#     \"3///, 3\\\\\\, 3|||, \\|/ baq,podo,3#,m·∫•y thg 3q ...\t üòÄüòÄ\",\n#     \"\\\\\\\\, 3que, parky, bake, 3#,\",\n#     \"th√®n dvm v·ªõi hcm ngoo nh∆∞ nhau h·∫øt\",\n#     \"d.m.c.s n√†y gh√™ qu√°aaaaaaaaaa @user #hashtag\",\n#     \"c.h.·∫ø.t\",\n#     \"c∆°m s∆∞·ªùn ∆°i s·∫Ωn s√®ng ch∆∞a dm cs\"\n# ]\n\n# print(\"=\"*80)\n# print(\"DEBUG: TESTING PREPROCESSING PIPELINE\")\n# print(\"=\"*80)\n\n# for i, text in enumerate(test_comments, 1):\n#     print(f\"\\n--- TEST {i} ---\")\n#     print(f\"Original: '{text}'\")\n    \n#     # Step 1: Unicode + lowercase\n#     step1 = normalize_unicode_lower(text)\n#     print(f\"Step 1:   '{step1}'\")\n    \n#     # Step 2a: Remove emoji/emoticon\n#     step2a = remove_emoji_emoticon(step1)\n#     print(f\"Step 2a:  '{step2a}'\")\n    \n#     # Step 2b: Remove HTML/URL/mentions\n#     step2b = remove_html_url_mention_hashtag(step2a)\n#     print(f\"Step 2b:  '{step2b}'\")\n    \n#     # Step 3: Reduce elongated\n#     step3 = reduce_elongated(step2b)\n#     print(f\"Step 3:   '{step3}'\")\n    \n#     # Step 4: Lexical normalization\n#     step4 = apply_lexical_normalization(step3)\n#     print(f\"Step 4:   '{step4}'\")\n    \n#     # Step 5: Remove punctuation\n#     step5 = remove_punctuation(step4)\n#     print(f\"Step 5:   '{step5}'\")\n    \n#     # Step 6: Strip spaces\n#     step6 = strip_extra_spaces(step5)\n#     print(f\"Final:    '{step6}'\")\n    \n#     if step6 == \"\":\n#         print(\"üö® WARNING: Text became empty!\")\n\n# print(\"\\n\" + \"=\"*80)\n# print(\"Dictionary status:\")\n# print(f\"Loaded entries: {len(norm_dict)}\")\n# if norm_dict:\n#     print(\"Sample dictionary entries:\")\n#     for k, v in list(norm_dict.items())[:5]:\n#         print(f\"  '{k}' ‚Üí '{v}'\")\n# print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:53.092391Z","iopub.execute_input":"2025-08-08T10:38:53.092654Z","iopub.status.idle":"2025-08-08T10:38:53.106594Z","shell.execute_reply.started":"2025-08-08T10:38:53.092627Z","shell.execute_reply":"2025-08-08T10:38:53.105795Z"}},"outputs":[],"execution_count":17}]}