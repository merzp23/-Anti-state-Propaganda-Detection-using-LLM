{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12564182,"sourceType":"datasetVersion","datasetId":7811352},{"sourceId":12709214,"sourceType":"datasetVersion","datasetId":7887000}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install libraries","metadata":{}},{"cell_type":"code","source":"# %%capture\n# !pip install emoji regex pandas unicodedata fasttext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.058504Z","iopub.execute_input":"2025-08-08T10:38:12.058806Z","iopub.status.idle":"2025-08-08T10:38:12.063893Z","shell.execute_reply.started":"2025-08-08T10:38:12.058785Z","shell.execute_reply":"2025-08-08T10:38:12.063209Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport regex\nimport emoji\nimport unicodedata\nimport uuid\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.065033Z","iopub.execute_input":"2025-08-08T10:38:12.065280Z","iopub.status.idle":"2025-08-08T10:38:12.395805Z","shell.execute_reply.started":"2025-08-08T10:38:12.065238Z","shell.execute_reply":"2025-08-08T10:38:12.394851Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# df = pd.read_csv(\"/kaggle/input/eureka-version-dataset/final_raw.csv\")\n# df = pd.read_csv(\"/kaggle/input/eureka-version-dataset/v2_raw.csv\")\ndf = pd.read_csv(\"/kaggle/input/eureka-version-dataset/final_raw_new.csv\")\nselected_df = df[['summary', 'comment_raw', 'label']]\nprint(\"======== Head ========\")\nprint(selected_df.head(5))\nprint(\"\\n======== Tail ========\")\nprint(selected_df.tail(5))\n\nprint(\"\\n======== Shape ========\")\nprint(selected_df.shape)\n\nprint(\"\\n======== Info ========\")\nprint(selected_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.396611Z","iopub.execute_input":"2025-08-08T10:38:12.396936Z","iopub.status.idle":"2025-08-08T10:38:12.782459Z","shell.execute_reply.started":"2025-08-08T10:38:12.396916Z","shell.execute_reply":"2025-08-08T10:38:12.781758Z"}},"outputs":[{"name":"stdout","text":"======== Head ========\n                                             summary  \\\n0  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n1  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n2  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n3  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n4  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n\n                                         comment_raw            label  \n0  luận điệu của bọn phản động, sỏ lá, 3/// viết ...  KHONG_PHAN_DONG  \n1  vậy ông bảo đại, ông diệm, ông thiệu là đảng v...  KHONG_PHAN_DONG  \n2                              muôn đời của đám 3///  KHONG_PHAN_DONG  \n3  già rồi mà đần vậy cháu ? cộng sản đánh mỹ, đá...  KHONG_PHAN_DONG  \n4  đúng là 3/// xỏ lá, bác hồ mất nên các bác khó...  KHONG_PHAN_DONG  \n\n======== Tail ========\n                                                 summary  \\\n17646  1. Nội dung sơ lược: Câu chuyện ngụ ngôn về cu...   \n17647  1. Nội dung sơ lược: Câu chuyện ngụ ngôn về cu...   \n17648  1. Nội dung sơ lược: Câu chuyện ngụ ngôn về cu...   \n17649  1. Nội dung sơ lược: Câu chuyện ngụ ngôn về cu...   \n17650  1. Nội dung sơ lược: Câu chuyện ngụ ngôn về cu...   \n\n                                             comment_raw            label  \n17646  ông bà nội bạn bị chôn sống thì tới đời con ch...  KHONG_LIEN_QUAN  \n17647                        vì đài dám nói lên sự thật?  KHONG_LIEN_QUAN  \n17648  sự thật gì? báo đài ngày nào chả đăng! thật cá...  KHONG_LIEN_QUAN  \n17649                   sự thật như nick của cháu vậy đó  KHONG_LIEN_QUAN  \n17650  con hoang được trại trẻ mồ côi nhận nuôi nó cò...  KHONG_LIEN_QUAN  \n\n======== Shape ========\n(17651, 3)\n\n======== Info ========\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 17651 entries, 0 to 17650\nData columns (total 3 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   summary      17651 non-null  object\n 1   comment_raw  17651 non-null  object\n 2   label        17651 non-null  object\ndtypes: object(3)\nmemory usage: 413.8+ KB\nNone\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 1. Normalize Unicode (NFC) + lowercase","metadata":{}},{"cell_type":"code","source":"def normalize_unicode_lower(text):\n    text = unicodedata.normalize('NFC', text)\n    return text.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.783750Z","iopub.execute_input":"2025-08-08T10:38:12.783966Z","iopub.status.idle":"2025-08-08T10:38:12.787609Z","shell.execute_reply.started":"2025-08-08T10:38:12.783949Z","shell.execute_reply":"2025-08-08T10:38:12.787004Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## 2. Remove Emoji, links/HTML tags/mentions/hashtags/UI indicators","metadata":{}},{"cell_type":"code","source":"# emoji\nEMOTICON_PATTERNS = [\n    r\":\\)+\",       # :), :)), :))), ...\n    r\":\\(+\",       # :(, :((, ...\n    r\":v+\",        # :v, :vvv, ...\n    r\":V+\",        # :V, :VV, ...\n    r\"=+\\)+\",      # =), =)), ...\n    r\"=+\\(+\",      # =(, =((, ...\n    r\":d+\",        # :d, :dd\n    r\":p+\",        # :p, :pp\n    r\"<3+\",        # <3<3<3\n    r\"=+\\]+\",      # =], =]], =]]], ...\n    r\"=+\\[+\",      # =[, =[[, =[[[ ...\n    r\":>+\",        # :>, :>>, ...\n    r\":<+\",        # :<, :<<, ...\n    r\":\\(\\(\",      # :((\n    r\"=\\(\\(\",      # =((\n]\n\nEMOTICON_REGEX = re.compile(\"|\".join(EMOTICON_PATTERNS), re.IGNORECASE)\n\nEMOJI_REGEX = re.compile(\n    \"[\"\n    u\"\\U0001F600-\\U0001F64F\"\n    u\"\\U0001F300-\\U0001F5FF\"\n    u\"\\U0001F680-\\U0001F6FF\"\n    u\"\\U0001F700-\\U0001F77F\"\n    u\"\\U0001F780-\\U0001F7FF\"\n    u\"\\U0001F800-\\U0001F8FF\"\n    u\"\\U0001F900-\\U0001F9FF\"\n    u\"\\U0001FA00-\\U0001FA6F\"\n    u\"\\U0001FA70-\\U0001FAFF\"\n    u\"\\U00002702-\\U000027B0\"\n    u\"\\U000024C2-\\U0001F251\" \n    \"]+\", flags=re.UNICODE\n)\n\ndef remove_emoji_emoticon(text):    \n    try:\n        text = emoji.replace_emoji(text, replace=\" \")\n    except:\n        text = EMOJI_REGEX.sub(\" \", text)\n    \n    # Xóa bằng regex\n    text = EMOTICON_REGEX.sub(\" \", text)\n    \n    # Loại bỏ khoảng trắng dư thừa\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n# url, html, mention, hashtags, ui_indicators\nURL_REGEX = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|co|vn|io)(/\\S*)?')\nHTML_REGEX = re.compile(r\"<[^>]+>\")\nMENTION_REGEX = re.compile(r\"@[\\w\\._]+\")\nHASHTAG_REGEX = re.compile(r\"#\\w+\")\nUI_INDICATORS = [\n    \"đã chỉnh sửa\", \"[đã chỉnh sửa]\", \"(đã chỉnh sửa)\",\n    \"see more\", \"xem thêm\", \"see translation\", \"xem bản dịch\",\n    \"ẩn bớt\", \"xem ít hơn\", \"dịch\", \"translated\", \"more\", \"less\",\n    \"see more reactions\"\n]\n\ndef remove_html_url_mention_hashtag(text):\n    if not isinstance(text, str):\n        return \"\"\n\n    # Xóa URL\n    text = URL_REGEX.sub(\" \", text)\n    \n    # Xóa HTML tags\n    text = HTML_REGEX.sub(\" \", text)\n    \n    # Xóa mentions và hashtags\n    text = MENTION_REGEX.sub(\" \", text)\n    text = HASHTAG_REGEX.sub(\" \", text)\n    \n    # Xóa ui_indicators\n    for ind in UI_INDICATORS:\n        text = re.sub(r'(?i)' + re.escape(ind), \" \", text)\n    \n    # Loại bỏ dấu câu riêng lẻ\n    text = re.sub(r'(?<!\\w)[\\^\\'\\`\\~\\\"\\,\\.]+(?!\\w)', ' ', text)\n    \n    # Làm sạch khoảng trắng dư thừa\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.788487Z","iopub.execute_input":"2025-08-08T10:38:12.788709Z","iopub.status.idle":"2025-08-08T10:38:12.811820Z","shell.execute_reply.started":"2025-08-08T10:38:12.788684Z","shell.execute_reply":"2025-08-08T10:38:12.811056Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 3. Reduce elongated characters","metadata":{}},{"cell_type":"code","source":"def reduce_elongated(text):\n    if not isinstance(text, str):\n        return \"\"\n    pattern = regex.compile(r\"([\\p{L}])\\1{2,}\", flags=regex.IGNORECASE)\n    return pattern.sub(r\"\\1\\1\", text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.812591Z","iopub.execute_input":"2025-08-08T10:38:12.812810Z","iopub.status.idle":"2025-08-08T10:38:12.829665Z","shell.execute_reply.started":"2025-08-08T10:38:12.812791Z","shell.execute_reply":"2025-08-08T10:38:12.828948Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 4. Lexical Normalization","metadata":{}},{"cell_type":"code","source":"import json, re\n\n# --- Load dict and build patterns ---\nwith open('/kaggle/input/dictionary/abbreviation_dictionary.json', encoding='utf-8') as f:\n    norm_dict = json.load(f)\n\nmixed, pure, words = [], [], []\nfor slang, std in norm_dict.items():\n    esc = re.escape(slang)\n    # classification\n    if re.fullmatch(r\"[^\\w\\s]+\", slang):\n        pure.append((esc, std))\n    elif re.search(r\"[^\\w\\s]\", slang) and re.search(r\"\\w\", slang):\n        mixed.append((esc, std))\n    else:\n        # add \\b for normal words\n        words.append((rf\"\\b{esc}\\b\", std))\n\n# sort desc\nfor lst in (mixed, pure, words):\n    lst.sort(key=lambda x: -len(x[0].replace(r\"\\b\",\"\")))\n\n# compile \n_patterns = [\n    (re.compile(pat, flags=re.IGNORECASE), std)\n    for pat, std in (mixed + pure + words)\n]\n\ndef apply_lexical_normalization(text):\n    if not isinstance(text, str):\n        return text\n    for regex, std in _patterns:\n        text = regex.sub(std, text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.830471Z","iopub.execute_input":"2025-08-08T10:38:12.830808Z","iopub.status.idle":"2025-08-08T10:38:12.869920Z","shell.execute_reply.started":"2025-08-08T10:38:12.830784Z","shell.execute_reply":"2025-08-08T10:38:12.869431Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 4.1. Debug lexical normalization ","metadata":{}},{"cell_type":"code","source":"# import json, re\n\n# # --- Load dict and build patterns ---\n# with open('/kaggle/input/dictionary/abbreviation_dictionary.json', encoding='utf-8') as f:\n#     norm_dict = json.load(f)\n\n# # --- Thêm tracking cho từ khóa chính trị nhạy cảm ---\n# POLITICAL_KEYWORDS = [\n#     'bò đỏ', 'ba que', 'phản động', 'dư luận viên',\n#     'cộng sản', 'việt cộng', 'độc tài', 'đảng',\n#     'việt nam cộng hòa', 'bắc kỳ', 'nam kỳ',\n#     'trung quốc', 'tàu cộng', 'yêu nước', 'tự do', 'nhân quyền',\n#     # Thêm các biến thể có thể có trong dictionary\n#     'cs', 'vc', 'vnch', 'tc', 'pd', 'dlv'\n# ]\n\n# # Tracking dict để ghi lại các transformations\n# political_transformations = {}\n# for keyword in POLITICAL_KEYWORDS:\n#     if keyword in norm_dict:\n#         political_transformations[keyword] = norm_dict[keyword]\n\n# print(\"=== POLITICAL KEYWORDS NORMALIZATION TRACKING ===\")\n# print(f\"Total dictionary entries: {len(norm_dict):,}\")\n# print(f\"Political keywords to track: {len(POLITICAL_KEYWORDS)}\")\n# print(f\"Political keywords found in dictionary: {len(political_transformations)}\")\n\n# if political_transformations:\n#     print(\"\\nPolitical keyword transformations:\")\n#     for original, normalized in political_transformations.items():\n#         print(f\"  '{original}' → '{normalized}'\")\n# else:\n#     print(\"\\nNo political keywords found in normalization dictionary\")\n\n# # Kiểm tra các từ khóa có pattern tương tự\n# similar_keys = []\n# for key in norm_dict.keys():\n#     for pol_word in POLITICAL_KEYWORDS:\n#         if pol_word.lower() in key.lower() or key.lower() in pol_word.lower():\n#             if key not in political_transformations:\n#                 similar_keys.append((key, norm_dict[key]))\n\n# if similar_keys:\n#     print(f\"\\nSimilar/related entries found: {len(similar_keys)}\")\n#     for original, normalized in similar_keys[:10]:  # Hiển thị top 10\n#         print(f\"  '{original}' → '{normalized}'\")\n#     if len(similar_keys) > 10:\n#         print(f\"  ... and {len(similar_keys) - 10} more\")\n\n# mixed, pure, words = [], [], []\n# for slang, std in norm_dict.items():\n#     esc = re.escape(slang)\n#     # classification\n#     if re.fullmatch(r\"[^\\w\\s]+\", slang):\n#         pure.append((esc, std))\n#     elif re.search(r\"[^\\w\\s]\", slang) and re.search(r\"\\w\", slang):\n#         mixed.append((esc, std))\n#     else:\n#         # add \\b for normal words\n#         words.append((rf\"\\b{esc}\\b\", std))\n\n# # sort desc\n# for lst in (mixed, pure, words):\n#     lst.sort(key=lambda x: -len(x[0].replace(r\"\\b\",\"\")))\n\n# # compile \n# _patterns = [\n#     (re.compile(pat, flags=re.IGNORECASE), std)\n#     for pat, std in (mixed + pure + words)\n# ]\n\n# def apply_lexical_normalization(text):\n#     if not isinstance(text, str):\n#         return text\n#     for regex, std in _patterns:\n#         text = regex.sub(std, text)\n#     return text\n\n# print(\"=== PATTERN COMPILATION COMPLETE ===\")\n# print(f\"Mixed patterns: {len(mixed)}\")\n# print(f\"Pure punctuation patterns: {len(pure)}\")\n# print(f\"Word patterns: {len(words)}\")\n# print(f\"Total compiled patterns: {len(_patterns)}\")\n# print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.870557Z","iopub.execute_input":"2025-08-08T10:38:12.870748Z","iopub.status.idle":"2025-08-08T10:38:12.875509Z","shell.execute_reply.started":"2025-08-08T10:38:12.870732Z","shell.execute_reply":"2025-08-08T10:38:12.874885Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 5. Remove punctuation","metadata":{}},{"cell_type":"code","source":"VIET_CHARACTERS = (\n    \"àáảãạăằắẳẵặâầấẩẫậ\"\n    \"èéẻẽẹêềếểễệ\"\n    \"ìíỉĩị\"\n    \"òóỏõọôồốổỗộơờớởỡợ\"\n    \"ùúủũụưừứửữự\"\n    \"ỳýỷỹỵ\"\n    \"đ\"\n)\n\ndef remove_punctuation(text):\n    if not isinstance(text, str):\n        return \"\"\n    # Giữ chữ Việt + chữ Anh + số + space\n    text = regex.sub(rf\"[^{VIET_CHARACTERS}a-zA-Z0-9\\s]+\", \" \", text)\n    return regex.sub(r\"\\s+\", \" \", text).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.877519Z","iopub.execute_input":"2025-08-08T10:38:12.877925Z","iopub.status.idle":"2025-08-08T10:38:12.891047Z","shell.execute_reply.started":"2025-08-08T10:38:12.877908Z","shell.execute_reply":"2025-08-08T10:38:12.890444Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 6. Whitespace Stripping ","metadata":{}},{"cell_type":"code","source":"def strip_extra_spaces(text):\n    if not isinstance(text, str):\n        return \"\"\n    return regex.sub(r\"\\s+\", \" \", text).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.891634Z","iopub.execute_input":"2025-08-08T10:38:12.891823Z","iopub.status.idle":"2025-08-08T10:38:12.903685Z","shell.execute_reply.started":"2025-08-08T10:38:12.891799Z","shell.execute_reply":"2025-08-08T10:38:12.903091Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 7. Deduplication ","metadata":{}},{"cell_type":"code","source":"def deduplicate_comments(df, col):\n    before = len(df)\n    df_nodup = df.drop_duplicates(subset=[col]).reset_index(drop=True)\n    after = len(df_nodup)\n    return df_nodup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.904428Z","iopub.execute_input":"2025-08-08T10:38:12.904656Z","iopub.status.idle":"2025-08-08T10:38:12.916481Z","shell.execute_reply.started":"2025-08-08T10:38:12.904641Z","shell.execute_reply":"2025-08-08T10:38:12.915894Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"selected_df = selected_df.copy()\noriginal_count = len(selected_df)\nprint(f\"Starting with {original_count:,} comments\")\n\n# 1. Unicode normalization + lowercase\nprint(\"1. Normalizing Unicode and converting to lowercase\")\nselected_df['comment_clean'] = selected_df['comment_raw'].apply(normalize_unicode_lower)\n\n# 2. Remove Emoji, links/HTML/mentions/hashtags/UI indicators\nprint(\"2. Removing Remove Emoji,links/HTML/mentions/hashtags/UI indicators\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_emoji_emoticon)\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_html_url_mention_hashtag)\n\n# 3. Reduce elongated characters\nprint(\"3. Reducing elongated characters\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(reduce_elongated)\n\n# 4. Lexical normalization\n# print(\"4. Applying lexical normalization\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(apply_lexical_normalization)\nprint(\"4. Applying lexical normalization\")\nbefore = selected_df['comment_clean'].copy()\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(apply_lexical_normalization)\n\nnum_lines_changed = (before != selected_df['comment_clean']).sum()\npercent_lines_changed = (num_lines_changed / len(selected_df)) * 100 if len(selected_df) else 0\n\nprint(f\"  Số dòng đã chỉnh sửa: {num_lines_changed:,}/{len(selected_df):,} ({percent_lines_changed:.2f}%)\")\n# 5. Remove punctuation\nprint(\"5. Removing all punctuation\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_punctuation)\n\n# 6. Whitespace Stripping\nprint(\"6. Whitespace Stripping\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(strip_extra_spaces)\n\n# 7. Deduplication\nprint(\"7. Removing duplicate comments\")\nbefore_dedup = len(selected_df)\nselected_df = deduplicate_comments(selected_df, col='comment_clean')\nafter_dedup = len(selected_df)\nprint(f\"  Removed {before_dedup - after_dedup:,} duplicate comments\")\n\n# Statistics Summary\nprint(\"\\n\" + \"=\"*50)\nfinal_count = len(selected_df)\ntotal_reduction = original_count - final_count\nretention_rate = (final_count / original_count) * 100\nreduction_rate = (total_reduction / original_count) * 100\n\nprint(f\"Original comments:      {original_count:,}\")\nprint(f\"After filtering:        {original_count:,} (100.0%)\")\nprint(f\"Final comments:         {final_count:,} ({retention_rate:.1f}%)\")\nprint(f\"Total reduction:        {total_reduction} comments ({reduction_rate:.1f}%)\")\n\n# Label Distribution\nif 'label' in selected_df.columns:\n    print(f\"\\n Label Distribution:\")\n    label_counts = selected_df['label'].value_counts().sort_index()\n    \n    for label, count in label_counts.items():\n        percentage = (count / final_count) * 100\n        print(f\"  • {label:<18}: {count:,} ({percentage:.1f}%)\")\n\nprint(\"=\"*50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:12.917127Z","iopub.execute_input":"2025-08-08T10:38:12.917359Z","iopub.status.idle":"2025-08-08T10:38:52.629160Z","shell.execute_reply.started":"2025-08-08T10:38:12.917334Z","shell.execute_reply":"2025-08-08T10:38:52.628529Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Starting with 17,651 comments\n1. Normalizing Unicode and converting to lowercase\n2. Removing Remove Emoji,links/HTML/mentions/hashtags/UI indicators\n3. Reducing elongated characters\n4. Applying lexical normalization\n  Số dòng đã chỉnh sửa: 9,517/17,651 (53.92%)\n5. Removing all punctuation\n6. Whitespace Stripping\n7. Removing duplicate comments\n  Removed 350 duplicate comments\n\n==================================================\nOriginal comments:      17,651\nAfter filtering:        17,651 (100.0%)\nFinal comments:         17,301 (98.0%)\nTotal reduction:        350 comments (2.0%)\n\n Label Distribution:\n  • KHONG_LIEN_QUAN   : 8,886 (51.4%)\n  • KHONG_PHAN_DONG   : 6,202 (35.8%)\n  • PHAN_DONG         : 2,213 (12.8%)\n==================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"selected_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:52.629960Z","iopub.execute_input":"2025-08-08T10:38:52.630223Z","iopub.status.idle":"2025-08-08T10:38:52.642981Z","shell.execute_reply.started":"2025-08-08T10:38:52.630199Z","shell.execute_reply":"2025-08-08T10:38:52.642093Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 17301 entries, 0 to 17300\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   summary        17301 non-null  object\n 1   comment_raw    17301 non-null  object\n 2   label          17301 non-null  object\n 3   comment_clean  17301 non-null  object\ndtypes: object(4)\nmemory usage: 540.8+ KB\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### DEBUG","metadata":{}},{"cell_type":"code","source":"# selected_df = selected_df.copy()\n# original_count = len(selected_df)\n# print(f\"Starting with {original_count:,} comments\")\n\n# # 1. Unicode normalization + lowercase\n# print(\"1. Normalizing Unicode and converting to lowercase\")\n# selected_df['comment_clean'] = selected_df['comment_raw'].apply(normalize_unicode_lower)\n\n# # 2. Remove Emoji, links/HTML/mentions/hashtags/UI indicators\n# print(\"2. Removing Remove Emoji,links/HTML/mentions/hashtags/UI indicators\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_emoji_emoticon)\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_html_url_mention_hashtag)\n\n# # 3. Reduce elongated characters\n# print(\"3. Reducing elongated characters\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(reduce_elongated)\n\n# # 4. Lexical normalization với tracking chi tiết\n# print(\"4. Applying lexical normalization\")\n# before = selected_df['comment_clean'].copy()\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(apply_lexical_normalization)\n\n# num_lines_changed = (before != selected_df['comment_clean']).sum()\n# percent_lines_changed = (num_lines_changed / len(selected_df)) * 100 if len(selected_df) else 0\n\n# print(f\"  Số dòng đã chỉnh sửa: {num_lines_changed:,}/{len(selected_df):,} ({percent_lines_changed:.2f}%)\")\n\n# # Thêm tracking cho political keywords\n# print(\"\\n  === POLITICAL KEYWORDS TRACKING ===\")\n# political_keyword_stats = {}\n\n# for keyword in POLITICAL_KEYWORDS:\n#     before_count = before.str.contains(keyword, case=False, na=False).sum()\n#     after_count = selected_df['comment_clean'].str.contains(keyword, case=False, na=False).sum()\n    \n#     if before_count > 0 or after_count > 0:\n#         political_keyword_stats[keyword] = {\n#             'before': before_count,\n#             'after': after_count,\n#             'change': after_count - before_count\n#         }\n\n# if political_keyword_stats:\n#     print(f\"  Political keywords found in dataset:\")\n#     for keyword, stats in political_keyword_stats.items():\n#         if stats['before'] > 0 or stats['after'] > 0:\n#             change_sign = \"+\" if stats['change'] > 0 else \"\"\n#             print(f\"    '{keyword}': {stats['before']} → {stats['after']} ({change_sign}{stats['change']})\")\n# else:\n#     print(\"  No tracked political keywords found in dataset\")\n\n# # Kiểm tra từ khóa được normalize thành gì\n# if political_transformations:\n#     print(f\"\\n  Checking normalized political keywords:\")\n#     for original, normalized in political_transformations.items():\n#         normalized_count = selected_df['comment_clean'].str.contains(normalized, case=False, na=False).sum()\n#         if normalized_count > 0:\n#             print(f\"    '{normalized}' (from '{original}'): {normalized_count} occurrences\")\n\n# print(\"  =\" * 40)\n\n# # 5. Remove punctuation\n# print(\"5. Removing all punctuation\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_punctuation)\n\n# # 6. Whitespace Stripping\n# print(\"6. Whitespace Stripping\")\n# selected_df['comment_clean'] = selected_df['comment_clean'].apply(strip_extra_spaces)\n\n# # 7. Deduplication\n# print(\"7. Removing duplicate comments\")\n# before_dedup = len(selected_df)\n# selected_df = deduplicate_comments(selected_df, col='comment_clean')\n# after_dedup = len(selected_df)\n# print(f\"  Removed {before_dedup - after_dedup:,} duplicate comments\")\n\n# # Statistics Summary\n# print(\"\\n\" + \"=\"*50)\n# final_count = len(selected_df)\n# total_reduction = original_count - final_count\n# retention_rate = (final_count / original_count) * 100\n# reduction_rate = (total_reduction / original_count) * 100\n\n# print(f\"Original comments:      {original_count:,}\")\n# print(f\"After filtering:        {original_count:,} (100.0%)\")\n# print(f\"Final comments:         {final_count:,} ({retention_rate:.1f}%)\")\n# print(f\"Total reduction:        {total_reduction} comments ({reduction_rate:.1f}%)\")\n\n# # Label Distribution\n# if 'label' in selected_df.columns:\n#     print(f\"\\n Label Distribution:\")\n#     label_counts = selected_df['label'].value_counts().sort_index()\n    \n#     for label, count in label_counts.items():\n#         percentage = (count / final_count) * 100\n#         print(f\"  • {label:<18}: {count:,} ({percentage:.1f}%)\")\n\n# # Final political keywords summary với breakdown theo label\n# print(f\"\\n=== FINAL POLITICAL KEYWORDS SUMMARY ===\")\n# final_political_stats = {}\n# for keyword in POLITICAL_KEYWORDS:\n#     count = selected_df['comment_clean'].str.contains(keyword, case=False, na=False).sum()\n#     if count > 0:\n#         final_political_stats[keyword] = count\n\n# if final_political_stats:\n#     print(\"Political keywords in final dataset:\")\n#     for keyword, count in sorted(final_political_stats.items(), key=lambda x: x[1], reverse=True):\n#         percentage = (count / final_count) * 100\n#         print(f\"  '{keyword}': {count:,} occurrences ({percentage:.2f}%)\")\n        \n#         # Breakdown by label\n#         if 'label' in selected_df.columns:\n#             for label in selected_df['label'].unique():\n#                 label_count = selected_df[selected_df['label'] == label]['comment_clean'].str.contains(keyword, case=False, na=False).sum()\n#                 if label_count > 0:\n#                     label_total = len(selected_df[selected_df['label'] == label])\n#                     label_percentage = (label_count / label_total) * 100\n#                     print(f\"    - {label}: {label_count:,} ({label_percentage:.2f}%)\")\n# else:\n#     print(\"No political keywords found in final dataset\")\n\n# print(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:52.643988Z","iopub.execute_input":"2025-08-08T10:38:52.644331Z","iopub.status.idle":"2025-08-08T10:38:52.659010Z","shell.execute_reply.started":"2025-08-08T10:38:52.644302Z","shell.execute_reply":"2025-08-08T10:38:52.658308Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Export\noutput_path = \"/kaggle/working/final_claened_new.csv\"\nselected_df.to_csv(output_path, index=False)\nprint(f\"Cleaned comments exported to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:52.659889Z","iopub.execute_input":"2025-08-08T10:38:52.660139Z","iopub.status.idle":"2025-08-08T10:38:53.086793Z","shell.execute_reply.started":"2025-08-08T10:38:52.660114Z","shell.execute_reply":"2025-08-08T10:38:53.086018Z"}},"outputs":[{"name":"stdout","text":"Cleaned comments exported to: /kaggle/working/final_claened_new.csv\n","output_type":"stream"}],"execution_count":15},{"cell_type":"raw","source":"## rename","metadata":{},"attachments":{}},{"cell_type":"code","source":"# # rename comment_clean -> comment\n# export_df = selected_df[['summary', 'comment_clean', 'label']] \\\n#     .rename(columns={'comment_clean': 'comment'})\n\n# output_path = \"/kaggle/working/v2_clean_1.csv\"\n# export_df.to_csv(output_path, index=False)\n\n# print(f\"Cleaned comments exported to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:53.087661Z","iopub.execute_input":"2025-08-08T10:38:53.087997Z","iopub.status.idle":"2025-08-08T10:38:53.091689Z","shell.execute_reply.started":"2025-08-08T10:38:53.087972Z","shell.execute_reply":"2025-08-08T10:38:53.090913Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# # test\n# # DEBUG\n# test_comments = [\n#     \"3///, 3\\\\\\, 3|||, \\|/ baq,podo,3#,mấy thg 3q ...\t 😀😀\",\n#     \"\\\\\\\\, 3que, parky, bake, 3#,\",\n#     \"thèn dvm với hcm ngoo như nhau hết\",\n#     \"d.m.c.s này ghê quáaaaaaaaaaa @user #hashtag\",\n#     \"c.h.ế.t\",\n#     \"cơm sườn ơi sẽn sèng chưa dm cs\"\n# ]\n\n# print(\"=\"*80)\n# print(\"DEBUG: TESTING PREPROCESSING PIPELINE\")\n# print(\"=\"*80)\n\n# for i, text in enumerate(test_comments, 1):\n#     print(f\"\\n--- TEST {i} ---\")\n#     print(f\"Original: '{text}'\")\n    \n#     # Step 1: Unicode + lowercase\n#     step1 = normalize_unicode_lower(text)\n#     print(f\"Step 1:   '{step1}'\")\n    \n#     # Step 2a: Remove emoji/emoticon\n#     step2a = remove_emoji_emoticon(step1)\n#     print(f\"Step 2a:  '{step2a}'\")\n    \n#     # Step 2b: Remove HTML/URL/mentions\n#     step2b = remove_html_url_mention_hashtag(step2a)\n#     print(f\"Step 2b:  '{step2b}'\")\n    \n#     # Step 3: Reduce elongated\n#     step3 = reduce_elongated(step2b)\n#     print(f\"Step 3:   '{step3}'\")\n    \n#     # Step 4: Lexical normalization\n#     step4 = apply_lexical_normalization(step3)\n#     print(f\"Step 4:   '{step4}'\")\n    \n#     # Step 5: Remove punctuation\n#     step5 = remove_punctuation(step4)\n#     print(f\"Step 5:   '{step5}'\")\n    \n#     # Step 6: Strip spaces\n#     step6 = strip_extra_spaces(step5)\n#     print(f\"Final:    '{step6}'\")\n    \n#     if step6 == \"\":\n#         print(\"🚨 WARNING: Text became empty!\")\n\n# print(\"\\n\" + \"=\"*80)\n# print(\"Dictionary status:\")\n# print(f\"Loaded entries: {len(norm_dict)}\")\n# if norm_dict:\n#     print(\"Sample dictionary entries:\")\n#     for k, v in list(norm_dict.items())[:5]:\n#         print(f\"  '{k}' → '{v}'\")\n# print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T10:38:53.092391Z","iopub.execute_input":"2025-08-08T10:38:53.092654Z","iopub.status.idle":"2025-08-08T10:38:53.106594Z","shell.execute_reply.started":"2025-08-08T10:38:53.092627Z","shell.execute_reply":"2025-08-08T10:38:53.105795Z"}},"outputs":[],"execution_count":17}]}